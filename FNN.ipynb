{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d47ace34-f3db-451f-9dad-d6a7b6b7e400",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Activation(str, Enum):\n",
    "    SIGMOID = \"Sigmoid\"\n",
    "    SOFTMAX = \"Softmax\"\n",
    "    RELU = \"ReLU\"\n",
    "\n",
    "class Loss(str, Enum):\n",
    "    MSE = \"Mean Squared Error \"\n",
    "    CE = \"Cross Entropy\"\n",
    "    \n",
    "class Layer:\n",
    "    def __init__(self, neurons, activation):\n",
    "        self.activation = activation\n",
    "        self.w = None        \n",
    "        self.b = np.random.uniform(low=0, high=1, size=(neurons, 1))\n",
    "        self.z = None\n",
    "        self.a = None\n",
    "        self.delta = []\n",
    "        self.gradient_w= []\n",
    "        self.gradient_b= []\n",
    "        \n",
    "    def neuronsCount(self):\n",
    "        return self.b.size        \n",
    "        \n",
    "class FNN:\n",
    "    def __init__(self, lossFunction, inputs):\n",
    "        self.layers = []\n",
    "        self.inputsNodes = 0\n",
    "        self.next_index = 0\n",
    "        self.lostHist = []\n",
    "        self.x_train = None\n",
    "        self.batch_size = 1   \n",
    "        self.loss = []\n",
    "        self.inputsNodes = inputs\n",
    "        self.x_normalizeScalars = None\n",
    "        self.y_normalizeScalars = None\n",
    "        self.axisToNormalize = (None, None)\n",
    "        self.lossFunction = lossFunction\n",
    "        \n",
    "    def activation(self, activ, z):\n",
    "        if(activ == Activation.SIGMOID):\n",
    "            return self.sigmoid(z)\n",
    "        elif(activ == Activation.SOFTMAX):\n",
    "            return self.softmax(z)\n",
    "        else:\n",
    "            return self.relu(z)\n",
    "            \n",
    "    def activation_derivative(self, activ, z):\n",
    "        if(activ == Activation.SIGMOID):\n",
    "            return self.sigmoid_derivative(z)\n",
    "        elif(activ == Activation.RELU):\n",
    "            return self.relu_derivative(z)\n",
    "        else:\n",
    "            return self.softmax_derivative(z)\n",
    "            \n",
    "    def MSE(self, a, y):\n",
    "        return np.square((y-a))/2  \n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        return 1/(1 + np.exp(-z))    \n",
    "    \n",
    "    def sigmoid_derivative(self, z):\n",
    "        return self.sigmoid(z) * (1-self.sigmoid(z))\n",
    "    \n",
    "    def relu(self, z):\n",
    "        return np.maximum(z,0)\n",
    "    \n",
    "    def relu_derivative(self, z):\n",
    "        return (z >= 0).astype(int)  \n",
    "    \n",
    "    def softmax(self, z):\n",
    "        return np.exp(z) / np.exp(z).sum(axis=0)\n",
    "    \n",
    "    def softmax_derivative(self, z):\n",
    "        return self.softmax(z) * (1 - self.softmax(z))\n",
    "    \n",
    "    def crossEntropy(self, a, y):\n",
    "        return -np.sum(y * np.log(a), axis = 0)\n",
    "    \n",
    "    def addLayer(self, activation, neurons):\n",
    "        layer = Layer(neurons, activation)\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def addInputs(self, inputsCount):\n",
    "        self.inputsNodes = inputsCount\n",
    "        \n",
    "    def set_weights(self, initValue = 1):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i == 0 :\n",
    "                layer.w = np.random.uniform(low=0, high=initValue, size=(self.inputsNodes,layer.neuronsCount()))\n",
    "            else:\n",
    "                layer.w = np.random.uniform(low=0, high=initValue, size=(self.layers[i-1].neuronsCount(),layer.neuronsCount()))\n",
    "            \n",
    "    def evaluateOne(self, x_batch):\n",
    "        for i,l in enumerate(self.layers):\n",
    "            # Logit\n",
    "            if i == 0:\n",
    "                l.z = np.dot(l.w.T, x_batch) + l.b\n",
    "            else:\n",
    "                l.z = np.dot(l.w.T, self.layers[i-1].a) + l.b\n",
    "            # Activation\n",
    "            l.a = self.activation(l.activation, l.z)\n",
    "  \n",
    "\n",
    "    def backpropagation(self, x_batch, y_batch, rate):\n",
    "        for j,l in enumerate(self.layers[::-1]):\n",
    "            i = len(self.layers) - (1 + j)\n",
    "            if i == len(self.layers) -1:\n",
    "                if self.lossFunction == Loss.CE:\n",
    "                    l.delta = l.a - y_batch\n",
    "                else:\n",
    "                    l.delta = (l.a - y_batch) * self.activation_derivative(l.activation, l.a)\n",
    "                l.gradient_w = np.einsum('ij,jk->jik', self.layers[i-1].a, l.delta.T)\n",
    "                l.gradient_w = np.mean(l.gradient_w, axis=0)\n",
    "                l.gradient_b = np.mean(l.delta, axis=1)\n",
    "            elif i == 0:\n",
    "                l.delta = np.multiply(self.activation_derivative(l.activation, l.z), np.dot(self.layers[i+1].w, self.layers[i+1].delta))\n",
    "                l.gradient_w= np.einsum('ij,jk->jik', x_batch, l.delta.T)\n",
    "                l.gradient_w = np.mean(l.gradient_w, axis=0)\n",
    "                l.gradient_b = np.mean(l.delta, axis=1)\n",
    "            else:\n",
    "                l.delta = np.multiply(self.activation_derivative(l.activation, l.z), np.dot(self.layers[i+1].w, self.layers[i+1].delta))\n",
    "                l.gradient_w= np.einsum('ij,jk->jik', self.layers[i-1].a, l.delta.T)\n",
    "                l.gradient_w = np.mean(l.gradient_w, axis=0)\n",
    "                l.gradient_b = np.mean(l.delta, axis=1)\n",
    "            \n",
    "        for l in self.layers:\n",
    "            l.gradient_b = l.gradient_b.reshape(len(l.gradient_b), 1)\n",
    "            l.w -= rate*l.gradient_w\n",
    "            l.b -= rate*l.gradient_b\n",
    "            \n",
    "    def train(self, x_train, y_train, epochs, rate, batch_size = 1):\n",
    "        self.set_weights()        \n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train   \n",
    "\n",
    "        if self.axisToNormalize[0] != None or self.axisToNormalize[1] != None:\n",
    "            print(\"X normalization : {} | Y normalization : {}\".format(self.axisToNormalize[0], self.axisToNormalize[1]))\n",
    "            self.normalizeBackend(self.axisToNormalize[0], self.axisToNormalize[1])\n",
    "        else :\n",
    "            print(\"[Warning] Data not normalized\")\n",
    "            \n",
    "        self.batch_size = batch_size   \n",
    "        \n",
    "        shuffleTime = int(np.ceil(len(self.x_train)/self.batch_size))        \n",
    "       \n",
    "        for epoch in range(epochs):\n",
    "            idx = np.random.permutation(self.x_train.index)\n",
    "            self.x_train = self.x_train.reindex(idx).reset_index(drop=True, inplace=False)\n",
    "            self.y_train = self.y_train.reindex(idx).reset_index(drop=True, inplace=False)\n",
    "            self.next_index = 0\n",
    "            epochLoss = []            \n",
    "            for it in range(shuffleTime):            \n",
    "                current_index = self.next_index\n",
    "                self.next_index = self.next_index + self.batch_size if self.next_index + self.batch_size < len(self.x_train) - 1 else 0\n",
    "\n",
    "                x_batch = self.x_train.iloc[current_index:].to_numpy().T if self.next_index == 0 else self.x_train.iloc[current_index: self.next_index].to_numpy().T\n",
    "                y_batch = np.vstack(self.y_train.iloc[current_index:].to_numpy()).T if self.next_index == 0 else np.vstack(self.y_train.iloc[current_index:self.next_index].to_numpy()).T        \n",
    "\n",
    "                self.evaluateOne(x_batch)\n",
    "                \n",
    "                if self.lossFunction == Loss.CE:\n",
    "                    loss = np.mean(self.crossEntropy(self.layers[-1].a, y_batch))\n",
    "                else:\n",
    "                    loss = np.mean(self.MSE(self.layers[-1].a, y_batch))    \n",
    "                    \n",
    "                epochLoss.append(loss)\n",
    "                self.backpropagation(x_batch, y_batch, rate)\n",
    "            if(epoch% 100 == 0 or epoch == epochs-1):\n",
    "                print(\"Epoch {} / {} - Loss : {}\".format(epoch+1, epochs, np.mean(epochLoss)))\n",
    "            self.loss.append(np.mean(epochLoss))\n",
    "            \n",
    "    def normalizeBackend(self, x, y):\n",
    "        if x is True:\n",
    "            self.x_normalizeScalars = (self.x_train.min(axis=0), self.x_train.max(axis=0))\n",
    "            self.x_train = (self.x_train - self.x_normalizeScalars[0]) / (self.x_normalizeScalars[1] - self.x_normalizeScalars[0])\n",
    "        if y is True:\n",
    "            self.y_normalizeScalars = (self.y_train.min(axis=0), self.y_train.max(axis=0))\n",
    "            self.y_train = (self.y_train - self.y_normalizeScalars[0]) / (self.y_normalizeScalars[1] - self.y_normalizeScalars[0])\n",
    "            \n",
    "    def normalize(self, x=True, y=False):\n",
    "        self.axisToNormalize = (x,y)\n",
    "    \n",
    "    def predict(self, x_batch, returnValue = False):\n",
    "        if self.axisToNormalize[0] != None:\n",
    "            print(\"X normalization : {} \".format(self.axisToNormalize[0]))\n",
    "            x_batch = (x_batch - self.x_normalizeScalars[0]) / (self.x_normalizeScalars[1] - self.x_normalizeScalars[0])\n",
    "        x_batch_T = x_batch.to_numpy().T\n",
    "        self.evaluateOne( x_batch_T)\n",
    "        if returnValue:\n",
    "            return self.layers[-1].a\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    def test(self, y, y_pred = None):\n",
    "        y_original = y.copy()\n",
    "        y_original = np.stack(y_original.to_numpy()).T\n",
    "\n",
    "        if(y_pred == None):\n",
    "            y_pred = self.layers[-1].a\n",
    "        # Transpose Y (observations in columns)\n",
    "        if self.axisToNormalize[1] is True:\n",
    "            print(\"Y normalization : {} \".format(self.axisToNormalize[1]))\n",
    "            y = (y - self.y_normalizeScalars[0]) / (self.y_normalizeScalars[1] - self.y_normalizeScalars[0])\n",
    "        y = np.stack(y.to_numpy()).T\n",
    "        \n",
    "        if self.lossFunction == Loss.CE:\n",
    "            # Good prediciton rate\n",
    "            max_indices = np.argmax(y_pred, axis=0)\n",
    "            matching_counts = np.sum(y[max_indices, np.arange(y_pred.shape[1])])\n",
    "            print(\"Good prediction : {}/{} | {}%\".format(matching_counts, len(max_indices), (matching_counts * 100) / len(max_indices)))\n",
    "            print(\"Loss : {}\\n\".format(np.mean(self.crossEntropy(y_pred, y))))\n",
    "        else:\n",
    "\n",
    "            df_test = pd.DataFrame({'Ypred': y_pred[0], 'YobsNormalized': y})\n",
    "            df_test[\"difNormalized\"] = y_pred[0] - y\n",
    "            df_test[\"YpredDeNormalized\"] = y_pred[0] * (self.y_normalizeScalars[1] -self.y_normalizeScalars[0]) + self.y_normalizeScalars[0]\n",
    "            df_test[\"Yobs\"] = y_original\n",
    "            df_test[\"difDeNormalized\"] = df_test[\"YpredDeNormalized\"] - df_test[\"Yobs\"]\n",
    "            df_test[\"loss\"] = self.MSE(y_pred, y)[0]            \n",
    "            display(df_test)\n",
    "            print(\"Loss : {}\\n\".format(np.mean(self.MSE(y_pred, y))))\n",
    "        # Plot loss over epochs\n",
    "        plt.plot(model.loss)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Loss over epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6907ce63-89e1-4789-b34a-8778f8850db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "np.set_printoptions(linewidth=np.inf)\n",
    "\n",
    "def combine_to_list(row):\n",
    "    return np.array([row['Iris-setosa'], row['Iris-versicolor'], row['Iris-virginica']])\n",
    "\n",
    "# Load csv\n",
    "df = pd.read_csv(\"iris_csv.csv\")\n",
    "\n",
    "# One hot econding class\n",
    "classCols = pd.get_dummies(df['class'], prefix='', prefix_sep='')\n",
    "df = pd.concat([df, classCols.apply(combine_to_list, axis = 1)], axis=1)\n",
    "df.drop(['class'], axis=1, inplace=True)\n",
    "\n",
    "# shuffle before split\n",
    "df = df.sample(frac = 1).reset_index(drop=True, inplace=False)\n",
    "\n",
    "# Split train/test\n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "train, test = df[msk], df[~msk]\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Split training data between X and Y\n",
    "\n",
    "#(split like this for a classification task)\n",
    "#y_train = train[0]\n",
    "#x_train = train.iloc[:, :-1]\n",
    "\n",
    "# (split like this for a regresion task)\n",
    "x_train = train[[\"sepallength\", \"sepalwidth\", \"petalwidth\"]]\n",
    "y_train = train.petallength\n",
    "\n",
    "# Split test data between X and Y\n",
    "\n",
    "#(split like this for a classification task)\n",
    "#y_test = test[0]\n",
    "#x_test = test.iloc[:, :-1]\n",
    "\n",
    "# (split like this for a regresion task)\n",
    "y_test = test.petallength\n",
    "x_test = test[[\"sepallength\", \"sepalwidth\", \"petalwidth\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21e3ac30-e1bc-4d6a-8705-a478065a610b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X normalization : True | Y normalization : True\n",
      "Epoch 1 / 10000 - Loss : 0.11186551131450523\n",
      "Epoch 101 / 10000 - Loss : 0.045000857827336864\n",
      "Epoch 201 / 10000 - Loss : 0.04463943273195982\n",
      "Epoch 301 / 10000 - Loss : 0.0444330872049954\n",
      "Epoch 401 / 10000 - Loss : 0.04421522290468608\n",
      "Epoch 501 / 10000 - Loss : 0.04398908402472048\n",
      "Epoch 601 / 10000 - Loss : 0.04375612281373016\n",
      "Epoch 701 / 10000 - Loss : 0.04349918070655827\n",
      "Epoch 801 / 10000 - Loss : 0.04322830422141712\n",
      "Epoch 901 / 10000 - Loss : 0.04291760797969622\n",
      "Epoch 1001 / 10000 - Loss : 0.04257160644534298\n",
      "Epoch 1101 / 10000 - Loss : 0.04218433602857399\n",
      "Epoch 1201 / 10000 - Loss : 0.04172580262211499\n",
      "Epoch 1301 / 10000 - Loss : 0.04121369078013752\n",
      "Epoch 1401 / 10000 - Loss : 0.04062001617580301\n",
      "Epoch 1501 / 10000 - Loss : 0.039931576062434866\n",
      "Epoch 1601 / 10000 - Loss : 0.03912473074814373\n",
      "Epoch 1701 / 10000 - Loss : 0.03819010046072188\n",
      "Epoch 1801 / 10000 - Loss : 0.03709623191997967\n",
      "Epoch 1901 / 10000 - Loss : 0.035832163332626156\n",
      "Epoch 2001 / 10000 - Loss : 0.03436873292607555\n",
      "Epoch 2101 / 10000 - Loss : 0.032704909426887606\n",
      "Epoch 2201 / 10000 - Loss : 0.030832861522083647\n",
      "Epoch 2301 / 10000 - Loss : 0.028760375058791067\n",
      "Epoch 2401 / 10000 - Loss : 0.02653398034309663\n",
      "Epoch 2501 / 10000 - Loss : 0.024190116371745347\n",
      "Epoch 2601 / 10000 - Loss : 0.021797237866161345\n",
      "Epoch 2701 / 10000 - Loss : 0.019411783511736908\n",
      "Epoch 2801 / 10000 - Loss : 0.01711898066023977\n",
      "Epoch 2901 / 10000 - Loss : 0.014957907941825985\n",
      "Epoch 3001 / 10000 - Loss : 0.012979267826839374\n",
      "Epoch 3101 / 10000 - Loss : 0.011198418299388533\n",
      "Epoch 3201 / 10000 - Loss : 0.009633667915650903\n",
      "Epoch 3301 / 10000 - Loss : 0.008277925054553987\n",
      "Epoch 3401 / 10000 - Loss : 0.007112469139194203\n",
      "Epoch 3501 / 10000 - Loss : 0.006126386654580586\n",
      "Epoch 3601 / 10000 - Loss : 0.005300980853486218\n",
      "Epoch 3701 / 10000 - Loss : 0.004610350647954016\n",
      "Epoch 3801 / 10000 - Loss : 0.004038150411951228\n",
      "Epoch 3901 / 10000 - Loss : 0.003562621534224136\n",
      "Epoch 4001 / 10000 - Loss : 0.0031712685582992463\n",
      "Epoch 4101 / 10000 - Loss : 0.0028481156661787988\n",
      "Epoch 4201 / 10000 - Loss : 0.0025816738873263873\n",
      "Epoch 4301 / 10000 - Loss : 0.002361048535257705\n",
      "Epoch 4401 / 10000 - Loss : 0.0021794530996535263\n",
      "Epoch 4501 / 10000 - Loss : 0.0020298854290023752\n",
      "Epoch 4601 / 10000 - Loss : 0.001906727778217732\n",
      "Epoch 4701 / 10000 - Loss : 0.0018042012273008564\n",
      "Epoch 4801 / 10000 - Loss : 0.0017194979762018962\n",
      "Epoch 4901 / 10000 - Loss : 0.0016504330561959975\n",
      "Epoch 5001 / 10000 - Loss : 0.001593541450833814\n",
      "Epoch 5101 / 10000 - Loss : 0.0015454900413134107\n",
      "Epoch 5201 / 10000 - Loss : 0.0015055235435207745\n",
      "Epoch 5301 / 10000 - Loss : 0.0014733630648961086\n",
      "Epoch 5401 / 10000 - Loss : 0.0014465274430824925\n",
      "Epoch 5501 / 10000 - Loss : 0.0014238026711158836\n",
      "Epoch 5601 / 10000 - Loss : 0.0014057575378212156\n",
      "Epoch 5701 / 10000 - Loss : 0.0013904254119707189\n",
      "Epoch 5801 / 10000 - Loss : 0.0013776595256166258\n",
      "Epoch 5901 / 10000 - Loss : 0.001367486223026726\n",
      "Epoch 6001 / 10000 - Loss : 0.0013586112708828737\n",
      "Epoch 6101 / 10000 - Loss : 0.0013519651681956687\n",
      "Epoch 6201 / 10000 - Loss : 0.0013455782244598511\n",
      "Epoch 6301 / 10000 - Loss : 0.001340929085076912\n",
      "Epoch 6401 / 10000 - Loss : 0.0013371048713263958\n",
      "Epoch 6501 / 10000 - Loss : 0.0013338437029937914\n",
      "Epoch 6601 / 10000 - Loss : 0.0013312647124497222\n",
      "Epoch 6701 / 10000 - Loss : 0.0013286341265750818\n",
      "Epoch 6801 / 10000 - Loss : 0.001326806424143597\n",
      "Epoch 6901 / 10000 - Loss : 0.0013256134482474045\n",
      "Epoch 7001 / 10000 - Loss : 0.0013239516306566544\n",
      "Epoch 7101 / 10000 - Loss : 0.0013228062422470201\n",
      "Epoch 7201 / 10000 - Loss : 0.0013222431068750926\n",
      "Epoch 7301 / 10000 - Loss : 0.0013212086271542033\n",
      "Epoch 7401 / 10000 - Loss : 0.0013206089654981758\n",
      "Epoch 7501 / 10000 - Loss : 0.0013198303833188297\n",
      "Epoch 7601 / 10000 - Loss : 0.001319086329290694\n",
      "Epoch 7701 / 10000 - Loss : 0.001319196534708756\n",
      "Epoch 7801 / 10000 - Loss : 0.0013182365875113442\n",
      "Epoch 7901 / 10000 - Loss : 0.0013180738066194283\n",
      "Epoch 8001 / 10000 - Loss : 0.0013176089465115192\n",
      "Epoch 8101 / 10000 - Loss : 0.001317092478905051\n",
      "Epoch 8201 / 10000 - Loss : 0.001316810998052211\n",
      "Epoch 8301 / 10000 - Loss : 0.001316512386848994\n",
      "Epoch 8401 / 10000 - Loss : 0.001315907536503233\n",
      "Epoch 8501 / 10000 - Loss : 0.0013155626512210154\n",
      "Epoch 8601 / 10000 - Loss : 0.0013153503487784975\n",
      "Epoch 8701 / 10000 - Loss : 0.0013148519290175312\n",
      "Epoch 8801 / 10000 - Loss : 0.0013145993620982917\n",
      "Epoch 8901 / 10000 - Loss : 0.0013141443858103797\n",
      "Epoch 9001 / 10000 - Loss : 0.0013139939541026767\n",
      "Epoch 9101 / 10000 - Loss : 0.001313216256581519\n",
      "Epoch 9201 / 10000 - Loss : 0.0013126877979224743\n",
      "Epoch 9301 / 10000 - Loss : 0.001312467564788561\n",
      "Epoch 9401 / 10000 - Loss : 0.001311898712615693\n",
      "Epoch 9501 / 10000 - Loss : 0.0013115789561043351\n",
      "Epoch 9601 / 10000 - Loss : 0.0013111979247185212\n",
      "Epoch 9701 / 10000 - Loss : 0.0013106464887869416\n",
      "Epoch 9801 / 10000 - Loss : 0.0013104722398404325\n",
      "Epoch 9901 / 10000 - Loss : 0.0013101414856111421\n",
      "Epoch 10000 / 10000 - Loss : 0.0013093845845201454\n"
     ]
    }
   ],
   "source": [
    "model = FNN(Loss.MSE, inputs = 3)\n",
    "model.normalize(x = True, y = True)\n",
    "model.addLayer(Activation.SIGMOID, neurons = 3)\n",
    "model.addLayer(Activation.SIGMOID, neurons = 1)\n",
    "model.train(x_train, y_train, epochs = 10000, rate = .01, batch_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45dd5068-d356-4e28-96bc-da3a321ec662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X normalization : True \n"
     ]
    }
   ],
   "source": [
    "model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "356f2f4a-1e1b-44f9-acde-8ae6f92dd849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y normalization : True \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ypred</th>\n",
       "      <th>YobsNormalized</th>\n",
       "      <th>difNormalized</th>\n",
       "      <th>YpredDeNormalized</th>\n",
       "      <th>Yobs</th>\n",
       "      <th>difDeNormalized</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.831822</td>\n",
       "      <td>0.694915</td>\n",
       "      <td>0.136907</td>\n",
       "      <td>5.907752</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0.807752</td>\n",
       "      <td>0.009372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.733496</td>\n",
       "      <td>0.593220</td>\n",
       "      <td>0.140276</td>\n",
       "      <td>5.327626</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.827626</td>\n",
       "      <td>0.009839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.758979</td>\n",
       "      <td>0.694915</td>\n",
       "      <td>0.064063</td>\n",
       "      <td>5.477974</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0.377974</td>\n",
       "      <td>0.002052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.080741</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.012945</td>\n",
       "      <td>1.476373</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.076373</td>\n",
       "      <td>0.000084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.080293</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>-0.004453</td>\n",
       "      <td>1.473730</td>\n",
       "      <td>1.5</td>\n",
       "      <td>-0.026270</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.622572</td>\n",
       "      <td>0.627119</td>\n",
       "      <td>-0.004547</td>\n",
       "      <td>4.673174</td>\n",
       "      <td>4.7</td>\n",
       "      <td>-0.026826</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.806067</td>\n",
       "      <td>0.813559</td>\n",
       "      <td>-0.007492</td>\n",
       "      <td>5.755797</td>\n",
       "      <td>5.8</td>\n",
       "      <td>-0.044203</td>\n",
       "      <td>0.000028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.696975</td>\n",
       "      <td>0.661017</td>\n",
       "      <td>0.035958</td>\n",
       "      <td>5.112154</td>\n",
       "      <td>4.9</td>\n",
       "      <td>0.212154</td>\n",
       "      <td>0.000647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.096778</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>-0.004917</td>\n",
       "      <td>1.570988</td>\n",
       "      <td>1.6</td>\n",
       "      <td>-0.029012</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.084461</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>-0.017234</td>\n",
       "      <td>1.498322</td>\n",
       "      <td>1.6</td>\n",
       "      <td>-0.101678</td>\n",
       "      <td>0.000148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.068491</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.017643</td>\n",
       "      <td>1.404096</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.104096</td>\n",
       "      <td>0.000156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.786993</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.109027</td>\n",
       "      <td>5.643257</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.643257</td>\n",
       "      <td>0.005943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.435134</td>\n",
       "      <td>0.457627</td>\n",
       "      <td>-0.022493</td>\n",
       "      <td>3.567291</td>\n",
       "      <td>3.7</td>\n",
       "      <td>-0.132709</td>\n",
       "      <td>0.000253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.839119</td>\n",
       "      <td>0.864407</td>\n",
       "      <td>-0.025288</td>\n",
       "      <td>5.950801</td>\n",
       "      <td>6.1</td>\n",
       "      <td>-0.149199</td>\n",
       "      <td>0.000320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.751957</td>\n",
       "      <td>0.661017</td>\n",
       "      <td>0.090940</td>\n",
       "      <td>5.436543</td>\n",
       "      <td>4.9</td>\n",
       "      <td>0.536543</td>\n",
       "      <td>0.004135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.724405</td>\n",
       "      <td>0.694915</td>\n",
       "      <td>0.029490</td>\n",
       "      <td>5.273989</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0.173989</td>\n",
       "      <td>0.000435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.567675</td>\n",
       "      <td>0.508475</td>\n",
       "      <td>0.059200</td>\n",
       "      <td>4.349280</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.349280</td>\n",
       "      <td>0.001752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.090068</td>\n",
       "      <td>0.118644</td>\n",
       "      <td>-0.028576</td>\n",
       "      <td>1.531399</td>\n",
       "      <td>1.7</td>\n",
       "      <td>-0.168601</td>\n",
       "      <td>0.000408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.083552</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.015755</td>\n",
       "      <td>1.492955</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.092955</td>\n",
       "      <td>0.000124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.803963</td>\n",
       "      <td>0.694915</td>\n",
       "      <td>0.109048</td>\n",
       "      <td>5.743382</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0.643382</td>\n",
       "      <td>0.005946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.079581</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>-0.005164</td>\n",
       "      <td>1.469531</td>\n",
       "      <td>1.5</td>\n",
       "      <td>-0.030469</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.672272</td>\n",
       "      <td>0.694915</td>\n",
       "      <td>-0.022643</td>\n",
       "      <td>4.966407</td>\n",
       "      <td>5.1</td>\n",
       "      <td>-0.133593</td>\n",
       "      <td>0.000256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.642658</td>\n",
       "      <td>0.779661</td>\n",
       "      <td>-0.137003</td>\n",
       "      <td>4.791683</td>\n",
       "      <td>5.6</td>\n",
       "      <td>-0.808317</td>\n",
       "      <td>0.009385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.712619</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.034653</td>\n",
       "      <td>5.204450</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.204450</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.069021</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>-0.032674</td>\n",
       "      <td>1.407223</td>\n",
       "      <td>1.6</td>\n",
       "      <td>-0.192777</td>\n",
       "      <td>0.000534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.125012</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.074165</td>\n",
       "      <td>1.737572</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.437572</td>\n",
       "      <td>0.002750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.472437</td>\n",
       "      <td>0.576271</td>\n",
       "      <td>-0.103834</td>\n",
       "      <td>3.787379</td>\n",
       "      <td>4.4</td>\n",
       "      <td>-0.612621</td>\n",
       "      <td>0.005391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.702380</td>\n",
       "      <td>0.661017</td>\n",
       "      <td>0.041363</td>\n",
       "      <td>5.144040</td>\n",
       "      <td>4.9</td>\n",
       "      <td>0.244040</td>\n",
       "      <td>0.000855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.829456</td>\n",
       "      <td>0.864407</td>\n",
       "      <td>-0.034951</td>\n",
       "      <td>5.893789</td>\n",
       "      <td>6.1</td>\n",
       "      <td>-0.206211</td>\n",
       "      <td>0.000611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.730926</td>\n",
       "      <td>0.779661</td>\n",
       "      <td>-0.048735</td>\n",
       "      <td>5.312462</td>\n",
       "      <td>5.6</td>\n",
       "      <td>-0.287538</td>\n",
       "      <td>0.001188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.684367</td>\n",
       "      <td>0.576271</td>\n",
       "      <td>0.108096</td>\n",
       "      <td>5.037764</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.637764</td>\n",
       "      <td>0.005842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.714090</td>\n",
       "      <td>0.661017</td>\n",
       "      <td>0.053073</td>\n",
       "      <td>5.213128</td>\n",
       "      <td>4.9</td>\n",
       "      <td>0.313128</td>\n",
       "      <td>0.001408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.672092</td>\n",
       "      <td>0.694915</td>\n",
       "      <td>-0.022823</td>\n",
       "      <td>4.965345</td>\n",
       "      <td>5.1</td>\n",
       "      <td>-0.134655</td>\n",
       "      <td>0.000260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.095405</td>\n",
       "      <td>0.118644</td>\n",
       "      <td>-0.023239</td>\n",
       "      <td>1.562890</td>\n",
       "      <td>1.7</td>\n",
       "      <td>-0.137110</td>\n",
       "      <td>0.000270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.084407</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.050508</td>\n",
       "      <td>1.498000</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.298000</td>\n",
       "      <td>0.001276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Ypred  YobsNormalized  difNormalized  YpredDeNormalized  Yobs  \\\n",
       "0   0.831822        0.694915       0.136907           5.907752   5.1   \n",
       "1   0.733496        0.593220       0.140276           5.327626   4.5   \n",
       "2   0.758979        0.694915       0.064063           5.477974   5.1   \n",
       "3   0.080741        0.067797       0.012945           1.476373   1.4   \n",
       "4   0.080293        0.084746      -0.004453           1.473730   1.5   \n",
       "5   0.622572        0.627119      -0.004547           4.673174   4.7   \n",
       "6   0.806067        0.813559      -0.007492           5.755797   5.8   \n",
       "7   0.696975        0.661017       0.035958           5.112154   4.9   \n",
       "8   0.096778        0.101695      -0.004917           1.570988   1.6   \n",
       "9   0.084461        0.101695      -0.017234           1.498322   1.6   \n",
       "10  0.068491        0.050847       0.017643           1.404096   1.3   \n",
       "11  0.786993        0.677966       0.109027           5.643257   5.0   \n",
       "12  0.435134        0.457627      -0.022493           3.567291   3.7   \n",
       "13  0.839119        0.864407      -0.025288           5.950801   6.1   \n",
       "14  0.751957        0.661017       0.090940           5.436543   4.9   \n",
       "15  0.724405        0.694915       0.029490           5.273989   5.1   \n",
       "16  0.567675        0.508475       0.059200           4.349280   4.0   \n",
       "17  0.090068        0.118644      -0.028576           1.531399   1.7   \n",
       "18  0.083552        0.067797       0.015755           1.492955   1.4   \n",
       "19  0.803963        0.694915       0.109048           5.743382   5.1   \n",
       "20  0.079581        0.084746      -0.005164           1.469531   1.5   \n",
       "21  0.672272        0.694915      -0.022643           4.966407   5.1   \n",
       "22  0.642658        0.779661      -0.137003           4.791683   5.6   \n",
       "23  0.712619        0.677966       0.034653           5.204450   5.0   \n",
       "24  0.069021        0.101695      -0.032674           1.407223   1.6   \n",
       "25  0.125012        0.050847       0.074165           1.737572   1.3   \n",
       "26  0.472437        0.576271      -0.103834           3.787379   4.4   \n",
       "27  0.702380        0.661017       0.041363           5.144040   4.9   \n",
       "28  0.829456        0.864407      -0.034951           5.893789   6.1   \n",
       "29  0.730926        0.779661      -0.048735           5.312462   5.6   \n",
       "30  0.684367        0.576271       0.108096           5.037764   4.4   \n",
       "31  0.714090        0.661017       0.053073           5.213128   4.9   \n",
       "32  0.672092        0.694915      -0.022823           4.965345   5.1   \n",
       "33  0.095405        0.118644      -0.023239           1.562890   1.7   \n",
       "34  0.084407        0.033898       0.050508           1.498000   1.2   \n",
       "\n",
       "    difDeNormalized      loss  \n",
       "0          0.807752  0.009372  \n",
       "1          0.827626  0.009839  \n",
       "2          0.377974  0.002052  \n",
       "3          0.076373  0.000084  \n",
       "4         -0.026270  0.000010  \n",
       "5         -0.026826  0.000010  \n",
       "6         -0.044203  0.000028  \n",
       "7          0.212154  0.000647  \n",
       "8         -0.029012  0.000012  \n",
       "9         -0.101678  0.000148  \n",
       "10         0.104096  0.000156  \n",
       "11         0.643257  0.005943  \n",
       "12        -0.132709  0.000253  \n",
       "13        -0.149199  0.000320  \n",
       "14         0.536543  0.004135  \n",
       "15         0.173989  0.000435  \n",
       "16         0.349280  0.001752  \n",
       "17        -0.168601  0.000408  \n",
       "18         0.092955  0.000124  \n",
       "19         0.643382  0.005946  \n",
       "20        -0.030469  0.000013  \n",
       "21        -0.133593  0.000256  \n",
       "22        -0.808317  0.009385  \n",
       "23         0.204450  0.000600  \n",
       "24        -0.192777  0.000534  \n",
       "25         0.437572  0.002750  \n",
       "26        -0.612621  0.005391  \n",
       "27         0.244040  0.000855  \n",
       "28        -0.206211  0.000611  \n",
       "29        -0.287538  0.001188  \n",
       "30         0.637764  0.005842  \n",
       "31         0.313128  0.001408  \n",
       "32        -0.134655  0.000260  \n",
       "33        -0.137110  0.000270  \n",
       "34         0.298000  0.001276  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 0.002066111583602015\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.test(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d479c76-f177-4427-9de6-de893840eaa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
